---
title: "SML-AIMS-2"
author: "EPF"
date: "4/9/2018"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Statistical Machine Learning for Data Science -  Assignment 2

```{r results='hide', message=FALSE, warning=FALSE}
library(corrplot)
library(ggplot2) 
library(dplyr)
library(tidyr)
library(GGally)
```

## Read the data and view it
We look at the first observations to get a sense of the data

```{r, echo=FALSE}
XY <- read.csv('cars-1.csv')
head(XY)
```

## Generation of the requested plots

The command ggpairs() helps generate nice looking pairwise scatterplots

```{r, echo=FALSE}
ggpairs(XY)
```

## Computation of the correlation matrix

We compute the correlation matrix and then plot it using corrplot from the package corrplot

```{r, echo=FALSE}
corMat <- round(cor(XY),2)
print(corMat)
corrplot(corMat)
```

## Exercise 2

## Exploration weighting scheme w=1/k
```{r, echo=FALSE}

    source('input-scale-1.R')
    
    xy    <- read.csv('cars-1.csv')
 
    #xy    <- Boston[,ncol(Boston):1]
    #n     <- round(0.4*nrow(xy))
    #xy    <- xy[sample(nrow(xy))[1:n],]
    
    colnames(xy)[1]<-'Y'
  
    y    <- xy[, 1]
    x    <- xy[,-1]
    x    <- cubitize(x) 
    n    <- nrow(xy)
 

#  Function for kNearest Neighbors 

   kNN <- function(x, y, xnew, k=1)
   {
      nnew     <- nrow(xnew)
      yhat.knn <- numeric(nnew)

      for(i in 1:nnew)
      {   
        xn <- matrix(rep(t(xnew[i,]), times=nrow(x)), 
                     byrow=T, ncol=ncol(x))
        dist.x <- sqrt(rowSums((as.matrix(xn)-as.matrix(x))^2))
        s.dist <- sort(dist.x, index=T)
        neighbors <- s.dist$ix[1:k]
      
        # This is the equidistant weight. Default  
        w <- 1/k
      
        # This is the negative exponential weighting scheme, uncomment for use
        #w <- exp(-dist.x[neighbors])/sum(exp(-dist.x[neighbors]))
        
        #This is the inverse distance weighting scheme. Uncomment for use
        #w.x <- 1/(1+dist.x[neighbors]) 
        #w <- w.x/sum(w.x) 
        
        yhat.knn[i] <- sum(w*y[neighbors])        
      }
  
      return(list(yhat=yhat.knn)) 
   }  

#  Cross Validation (Leave One Out variety)

   K  <- n-1
   CV <- numeric(K)
 
   for(k in 1:K)
   { 
      yhat.cv <- numeric(n)
       
      for(i in 1:n)    
      {
         xy.cv   <- xy[-i, ]
         x.cv    <- xy.cv[,-1]
         y.cv    <- xy.cv[, 1]
         xnew    <- xy[i,-1]
         mod.knn <- kNN(x.cv, y.cv, xnew, k)
         yhat.cv[i] <- mod.knn$yhat
      } 
      CV[k] <- mean((y-yhat.cv)^2) 
   }

   k <- min(which(CV==min(CV)))
   
#  Explore all this now

   R      <- 100
   mse.te <- mse.tr <- matrix(0, nrow=R, ncol=2)

   for(r in 1:R)
   {
     id.tr <- sample(sample(n))[1:round(0.75*n)]
     xy.tr <- xy[ id.tr,]
     xy.te <- xy[-id.tr,] 

     knn.mod  <- kNN(xy.tr[,-1], xy.tr[,1], xy.tr[,-1], k=k)
     yhat.tr  <- knn.mod$yhat
     knn.mod  <- kNN(xy.tr[,-1], xy.tr[,1], xy.te[,-1], k=k)
     yhat.te  <- knn.mod$yhat

     mse.tr[r,1]  <- mean((xy.tr[,1]-yhat.tr)^2)    
     mse.te[r,1]  <- mean((xy.te[,1]-yhat.te)^2)
 
     lm.xy <- step(lm(Y~., data=xy.tr), ~., 
                   trace=0, direction='both')
     mse.tr[r,2]  <- mean((xy.tr[,1]-fitted(lm.xy))^2)
     mse.te[r,2]  <- mean((xy.te[,1]-predict(lm.xy, xy.te[,-1]))^2)     }
```

Behavior of Training and Test Errors  for w=1/k

```{r, echo=FALSE}
   par(mfrow=c(1,2))
   boxplot(mse.tr, names=c('kNN','MLR'), 
           main='Train Error')
   boxplot(mse.te, names=c('kNN','MLR'), 
           main='Test Error')
```

Cross validation on k for w=1/k

```{r, echo=FALSE}
   par(mfrow=c(1,1))
   plot(1:length(CV), CV, type='b', 
        col='red', xlab='k', ylab='CV(k)',
        main='CV for finding k in kNN regression')
```

Paired t-test for significance testing on true test errors

```{r, echo=FALSE}
   print(t.test(mse.te[,1], mse.te[,2], paired=T))
```

## Exercise: Bonus

```{r, echo=FALSE}
xy <- read.csv('prostate-cancer-1.csv')
barplot(prop.table(table(xy[,1])))
```

## Exercise 3

```{r, echo=FALSE}
 library(ElemStatLearn)
 data(zip.train)
 data(zip.test)

 xtrain <- zip.train[,-1]
 ytrain <- zip.train[, 1]
 xtest  <- zip.test[,-1]
 ytest  <- zip.test[, 1]
```
